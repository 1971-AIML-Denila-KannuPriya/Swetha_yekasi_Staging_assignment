{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97eaec15-6e36-41c3-a47d-a97ce3a5c339",
   "metadata": {},
   "source": [
    "# 1. Implement a K-Nearest Neighbors (KNN) Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0fa5d09-d08f-4ec4-8c3d-bf1c137677f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from math import sqrt\n",
    "\n",
    "def knn_classifier(data_points, new_point, k=3):\n",
    "    distances = []\n",
    "    for point in data_points:\n",
    "        distance = sqrt((point[0] - new_point[0]) ** 2 + (point[1] - new_point[1]) ** 2)\n",
    "        distances.append((distance, point[2]))\n",
    "    distances.sort(key=lambda x: x[0])\n",
    "    nearest_neighbors = [label for _, label in distances[:k]]\n",
    "    most_common = Counter(nearest_neighbors).most_common(1)\n",
    "    return most_common[0][0]\n",
    "    \n",
    "data = [(1, 2, 'A'), (2, 3, 'B'), (3, 1, 'A'), (6, 7, 'B'), (7, 6, 'B')]\n",
    "new_data_point = (5, 5)\n",
    "result = knn_classifier(data, new_data_point)\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adfdffa-e4c0-49cd-9d3c-4f6143d0ec94",
   "metadata": {},
   "source": [
    "#  2. Remove Outliers from Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20981164-557b-45fa-8df3-95df70c9fefb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 12, 13, 15, 22, 25, 30, 32, 35]\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "\n",
    "def remove_outliers(data):\n",
    "    mean = statistics.mean(data)\n",
    "    std_dev = statistics.stdev(data)\n",
    "    threshold = 2 * std_dev\n",
    "    return [x for x in data if abs(x - mean) <= threshold]\n",
    "    \n",
    "data = [10, 12, 13, 15, 22, 25, 500, 30, 32, 35]\n",
    "cleaned_data = remove_outliers(data)\n",
    "print(cleaned_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89caf78-ef0d-42c4-8dce-78fd8c4e4989",
   "metadata": {},
   "source": [
    "#  3. Optimize a Matrix Multiplication for Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b480db92-59d3-413e-812a-3e912a8c2a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19, 22], [43, 50]]\n"
     ]
    }
   ],
   "source": [
    "def matrix_multiply(mat1, mat2):\n",
    "    rows_mat1 = len(mat1)\n",
    "    cols_mat1 = len(mat1[0])\n",
    "    rows_mat2 = len(mat2)\n",
    "    cols_mat2 = len(mat2[0])\n",
    "\n",
    "    if cols_mat1 != rows_mat2:\n",
    "        return \"Incompatible matrices for multiplication\"\n",
    "\n",
    "    result = [[0 for _ in range(cols_mat2)] for _ in range(rows_mat1)]\n",
    "    \n",
    "    for i in range(rows_mat1):\n",
    "        for j in range(cols_mat2):\n",
    "            for k in range(cols_mat1):\n",
    "                result[i][j] += mat1[i][k] * mat2[k][j]\n",
    "    \n",
    "    return result\n",
    "    \n",
    "mat1 = [[1, 2], [3, 4]]\n",
    "mat2 = [[5, 6], [7, 8]]\n",
    "result = matrix_multiply(mat1, mat2)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d49096-eac1-406e-a091-e2a43ebebe65",
   "metadata": {},
   "source": [
    "# 4. Word Embedding Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18daef87-5717-464d-aa3e-906b4924d388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9746318461970762\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = sum(v1 * v2 for v1, v2 in zip(vec1, vec2))\n",
    "    magnitude_vec1 = math.sqrt(sum(v1 ** 2 for v1 in vec1))\n",
    "    magnitude_vec2 = math.sqrt(sum(v2 ** 2 for v2 in vec2))\n",
    "    return dot_product / (magnitude_vec1 * magnitude_vec2)\n",
    "vec1 = [1.0, 2.0, 3.0]\n",
    "vec2 = [4.0, 5.0, 6.0]\n",
    "similarity = cosine_similarity(vec1, vec2)\n",
    "print(similarity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f3198d-0229-486f-9de9-643252cf8093",
   "metadata": {},
   "source": [
    "#  5. Implement a Min-Heap Using a Priority Queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6c20bf4-5873-4aac-b524-120a4fbe558e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "5\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "\n",
    "class MinHeap:\n",
    "    def __init__(self):\n",
    "        self.heap = []\n",
    "\n",
    "    def insert(self, value):\n",
    "        heapq.heappush(self.heap, value)\n",
    "\n",
    "    def get_min(self):\n",
    "        return self.heap[0] if self.heap else None\n",
    "\n",
    "    def extract_min(self):\n",
    "        return heapq.heappop(self.heap) if self.heap else None\n",
    "        \n",
    "min_heap = MinHeap()\n",
    "\n",
    "min_heap.insert(10)\n",
    "min_heap.insert(5)\n",
    "min_heap.insert(30)\n",
    "\n",
    "print(min_heap.get_min())   \n",
    "print(min_heap.extract_min()) \n",
    "print(min_heap.get_min())    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62674b1e-f45a-4484-b6f9-ee389475adbb",
   "metadata": {},
   "source": [
    "# 6. Implement a Support Vector Machine (SVM) Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da90e492-5b02-4721-bb0c-91aa1cf93bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "def svm_classifier(data_points, new_point):\n",
    "    X = [(x, y) for x, y, label in data_points] \n",
    "    y = [label for _, _, label in data_points]  \n",
    "    \n",
    "    clf = svm.SVC(kernel='linear')  \n",
    "    clf.fit(X, y)                   \n",
    "    \n",
    "    return clf.predict([new_point])[0]  \n",
    "    \n",
    "data_points = [(1.0, 2.0, 'A'), (2.0, 3.0, 'A'), (3.0, 3.0, 'B'), (4.0, 5.0, 'B')]\n",
    "new_point = (3.5, 4.0)\n",
    "\n",
    "predicted_label = svm_classifier(data_points, new_point)\n",
    "print(predicted_label)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3f759e-897d-4126-9676-46c58536d526",
   "metadata": {},
   "source": [
    "#  7. Calculate the Z-Score of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a63ef17a-7a88-42af-bc35-b29c5be91b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.414213562373095, -0.7071067811865475, 0.0, 0.7071067811865475, 1.414213562373095]\n"
     ]
    }
   ],
   "source": [
    "def calculate_z_scores(data):\n",
    "    mean = sum(data) / len(data)\n",
    "    std_dev = (sum((x - mean) ** 2 for x in data) / len(data)) ** 0.5\n",
    "    return [(x - mean) / std_dev for x in data]\n",
    "data = [10, 20, 30, 40, 50]\n",
    "z_scores = calculate_z_scores(data)\n",
    "print(z_scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6527b857-af49-4457-a3ac-69e09adc7dba",
   "metadata": {},
   "source": [
    "#  8. K-Means Clustering Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fcd7b1f-a635-4bcd-81ba-49de90c46992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(10.5, 10.5), (3.0, 2.25)]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def euclidean_distance(p1, p2):\n",
    "    return ((p1[0] - p2[0]) ** 2 + (p1[1] - p2[1]) ** 2) ** 0.5\n",
    "\n",
    "def assign_clusters(data_points, centroids):\n",
    "    clusters = [[] for _ in centroids]\n",
    "    for point in data_points:\n",
    "        distances = [euclidean_distance(point, centroid) for centroid in centroids]\n",
    "        closest_centroid = distances.index(min(distances))\n",
    "        clusters[closest_centroid].append(point)\n",
    "    return clusters\n",
    "\n",
    "def calculate_centroids(clusters):\n",
    "    centroids = []\n",
    "    for cluster in clusters:\n",
    "        if len(cluster) > 0:\n",
    "            x_coords = [p[0] for p in cluster]\n",
    "            y_coords = [p[1] for p in cluster]\n",
    "            centroids.append((sum(x_coords) / len(x_coords), sum(y_coords) / len(y_coords)))\n",
    "    return centroids\n",
    "\n",
    "def k_means_clustering(data_points, k):\n",
    "    centroids = random.sample(data_points, k)\n",
    "    for _ in range(100):  # Max iterations\n",
    "        clusters = assign_clusters(data_points, centroids)\n",
    "        new_centroids = calculate_centroids(clusters)\n",
    "        if new_centroids == centroids:\n",
    "            break\n",
    "        centroids = new_centroids\n",
    "    return centroids\n",
    "data_points = [(1, 1), (2, 1), (4, 3), (5, 4), (10, 10), (11, 11)]\n",
    "k = 2\n",
    "centroids = k_means_clustering(data_points, k)\n",
    "print(centroids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19605995-d6b3-46a4-bcfc-66db795e9d8d",
   "metadata": {},
   "source": [
    "#  9. Evaluate Classification Model Using F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebaa2233-eac2-4416-96df-feb582c67374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8000000000000002\n"
     ]
    }
   ],
   "source": [
    "def f1_score(true_labels, predicted_labels):\n",
    "    tp = sum((t == 1 and p == 1) for t, p in zip(true_labels, predicted_labels))\n",
    "    fp = sum((t == 0 and p == 1) for t, p in zip(true_labels, predicted_labels))\n",
    "    fn = sum((t == 1 and p == 0) for t, p in zip(true_labels, predicted_labels))\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    \n",
    "    if precision + recall == 0:\n",
    "        return 0\n",
    "    \n",
    "    return 2 * (precision * recall) / (precision + recall)\n",
    "true_labels = [1, 0, 1, 1, 0, 1, 0, 0, 1]\n",
    "predicted_labels = [1, 0, 0, 1, 0, 1, 0, 1, 1]\n",
    "f1 = f1_score(true_labels, predicted_labels)\n",
    "print(f1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b101a8c-e8a9-4c14-8ddf-6e1656e7fc40",
   "metadata": {},
   "source": [
    "# 10. Visualize Data Distribution Using a Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85b80e7a-34ee-4b50-b48e-61013e95c8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1.20 - 2.08': 2, '2.08 - 2.96': 4, '2.96 - 3.84': 2, '3.84 - 4.72': 1, '4.72 - 5.60': 0}\n"
     ]
    }
   ],
   "source": [
    "def create_histogram(data, bins):\n",
    "    min_value = min(data)\n",
    "    max_value = max(data)\n",
    "    bin_width = (max_value - min_value) / bins\n",
    "    histogram = {}\n",
    "    \n",
    "    for i in range(bins):\n",
    "        bin_start = min_value + i * bin_width\n",
    "        bin_end = bin_start + bin_width\n",
    "        count = sum(1 for x in data if bin_start <= x < bin_end)\n",
    "        histogram[f\"{bin_start:.2f} - {bin_end:.2f}\"] = count\n",
    "        \n",
    "    return histogram\n",
    "data = [1.2, 2.3, 3.1, 2.8, 1.5, 3.7, 2.1, 4.0, 5.6, 2.5]\n",
    "bins = 5\n",
    "histogram = create_histogram(data, bins)\n",
    "print(histogram)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140f6849-ae5c-4bb8-a31e-285cc62402f4",
   "metadata": {},
   "source": [
    "# 11. Implement a Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb5c1485-76e9-4916-8c97-c7f0128f2038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n"
     ]
    }
   ],
   "source": [
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "\n",
    "def split(data, feature, threshold):\n",
    "    left = [point for point in data if point[0][feature] <= threshold]\n",
    "    right = [point for point in data if point[0][feature] > threshold]\n",
    "    return left, right\n",
    "\n",
    "def gini_index(data):\n",
    "    labels = [point[1] for point in data]\n",
    "    classes = set(labels)\n",
    "    total = len(labels)\n",
    "    gini = 1.0\n",
    "    for cls in classes:\n",
    "        prob = labels.count(cls) / total\n",
    "        gini -= prob ** 2\n",
    "    return gini\n",
    "\n",
    "def best_split(data):\n",
    "    best_gini = float('inf')\n",
    "    best_feature, best_threshold = None, None\n",
    "    for feature in range(len(data[0][0])):\n",
    "        thresholds = set(point[0][feature] for point in data)\n",
    "        for threshold in thresholds:\n",
    "            left, right = split(data, feature, threshold)\n",
    "            if left and right:\n",
    "                gini = (len(left) / len(data)) * gini_index(left) + (len(right) / len(data)) * gini_index(right)\n",
    "                if gini < best_gini:\n",
    "                    best_gini = gini\n",
    "                    best_feature = feature\n",
    "                    best_threshold = threshold\n",
    "    return best_feature, best_threshold\n",
    "\n",
    "def build_tree(data):\n",
    "    labels = [point[1] for point in data]\n",
    "    if len(set(labels)) == 1:\n",
    "        return Node(value=labels[0])\n",
    "    if not data:\n",
    "        return Node(value=max(set(labels), key=labels.count))\n",
    "\n",
    "    feature, threshold = best_split(data)\n",
    "    if feature is None:\n",
    "        return Node(value=max(set(labels), key=labels.count))\n",
    "\n",
    "    left_data, right_data = split(data, feature, threshold)\n",
    "    left_node = build_tree(left_data)\n",
    "    right_node = build_tree(right_data)\n",
    "    return Node(feature, threshold, left_node, right_node)\n",
    "\n",
    "def predict(tree, point):\n",
    "    if tree.value is not None:\n",
    "        return tree.value\n",
    "    if point[tree.feature] <= tree.threshold:\n",
    "        return predict(tree.left, point)\n",
    "    else:\n",
    "        return predict(tree.right, point)\n",
    "\n",
    "def decision_tree_classifier(data_points, new_point):\n",
    "    tree = build_tree(data_points)\n",
    "    return predict(tree, new_point)\n",
    "data_points = [([2.5, 3.5], 'A'), ([1.0, 1.5], 'B'), ([4.5, 5.5], 'A'), ([3.5, 2.5], 'B')]\n",
    "new_point = [3.0, 3.0]\n",
    "label = decision_tree_classifier(data_points, new_point)\n",
    "print(label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5f3ecf-2d5d-4e57-8e89-e23900470417",
   "metadata": {},
   "source": [
    "# 12. Normalize Data Using Min-Max Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d38976-c66f-411d-a1cd-3b865c1e63af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
